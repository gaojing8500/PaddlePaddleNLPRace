{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f97beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verify PaddlePaddle program ... \n",
      "PaddlePaddle works well on 1 GPU.\n",
      "PaddlePaddle works well on 1 GPUs.\n",
      "PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "##指向第四张卡，前三张在做训练\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import paddle\n",
    "paddle.utils.run_check()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2766428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-06-09 16:38:18,752] [    INFO]\u001b[0m - Loading token embedding...\u001b[0m\n",
      "\u001b[32m[2021-06-09 16:38:22,118] [    INFO]\u001b[0m - Finish loading embedding vector.\u001b[0m\n",
      "\u001b[32m[2021-06-09 16:38:22,122] [    INFO]\u001b[0m - Token Embedding info:             \n",
      "Unknown index: 352217             \n",
      "Unknown token: [UNK]             \n",
      "Padding index: 352218             \n",
      "Padding token: [PAD]             \n",
      "Shape :[352219, 300]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.embeddings import TokenEmbedding\n",
    "token_embedding = TokenEmbedding(embedding_name = \"w2v.wiki.target.word-word.dim300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52bcfa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object   type: TokenEmbedding(352219, 300, padding_idx=352218, sparse=False)             \n",
      "Unknown index: 352217             \n",
      "Unknown token: [UNK]             \n",
      "Padding index: 352218             \n",
      "Padding token: [PAD]             \n",
      "Parameter containing:\n",
      "Tensor(shape=[352219, 300], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [[ 0.15078500,  0.12059100, -0.22020400, ..., -0.53268701, -0.06691400, -0.10072800],\n",
      "        [-0.25135499,  0.23474200, -0.16972800, ..., -0.52722102, -0.22953101,  0.18078101],\n",
      "        [ 0.20782000,  0.14142500, -0.33728099, ..., -0.55558801,  0.04756100, -0.07089700],\n",
      "        ...,\n",
      "        [ 0.00247700, -0.00236600, -0.02842000, ..., -0.02777100, -0.01926100,  0.00767400],\n",
      "        [-0.00824524, -0.00958661,  0.01343321, ...,  0.01424389, -0.03027157, -0.00872602],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,  0.        ,  0.        ]])\n"
     ]
    }
   ],
   "source": [
    "print(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97a4ebd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.067538  0.189535  0.049175 -0.109212 -0.242945 -0.161946 -0.35808\n",
      "  -0.209951  0.05232  -0.104802  0.004568  0.040071  0.090112  0.121372\n",
      "   0.245141  0.353702 -0.003359 -0.080915 -0.048213  0.252324 -0.022077\n",
      "   0.197302  0.149266  0.39154   0.554964 -0.640223 -0.330378 -0.140783\n",
      "  -0.294932 -0.0542    0.38516   0.009788  0.080107 -0.315227  0.384387\n",
      "   0.257482  0.411435 -0.061848 -0.176585 -0.320802 -0.06256  -0.021686\n",
      "   0.185725 -0.030455 -0.040159  0.320913  0.305943 -0.035203 -0.29851\n",
      "   0.047758 -0.159635  0.043715 -0.1643    0.112271 -0.296435  0.132372\n",
      "  -0.234751 -0.306376 -0.22427  -0.186935 -0.08255  -0.211002  0.198403\n",
      "  -0.095023 -0.513217 -0.029123 -0.25614   0.205883  0.660782  0.12478\n",
      "  -0.162462 -0.568987 -0.178057 -0.001162  0.201736  0.273989  0.209728\n",
      "  -0.338654  0.088953  0.010972  0.59236  -0.478215 -0.365565 -0.127514\n",
      "   0.039382  0.091705 -0.540819  0.344238  0.322735  0.175959 -0.049957\n",
      "   0.032881  0.322046 -0.151226  0.168387  0.623019 -0.097401 -0.046273\n",
      "  -0.186942  0.556711 -0.072447  0.123356 -0.205667  0.406415  0.294187\n",
      "   0.147464 -0.507853  0.278789  0.190764  0.134965  0.168808 -0.188176\n",
      "   0.300241 -0.094525  0.270188 -0.066166 -0.505375  0.135869  0.143135\n",
      "  -0.219625  0.06112   0.22113  -0.301471 -0.253983 -0.221086 -0.06717\n",
      "  -0.382039  0.433792 -0.566812  0.519951 -0.226354 -0.091265 -0.667766\n",
      "  -0.285563 -0.032013  0.383694 -0.453125 -0.076344  0.048691 -0.571203\n",
      "   0.053848 -0.314853 -0.083938  0.366508  0.409597  0.041942 -0.40603\n",
      "  -0.147461  0.201303 -0.161571  0.084935 -0.713135  0.137806 -0.008691\n",
      "  -0.015997 -0.354195 -0.240789  0.179498 -0.599436 -0.529557  0.079033\n",
      "  -0.417115 -0.464429 -0.067881  0.02777   0.236006  0.031313  0.183077\n",
      "  -0.079289 -0.497515  0.071414 -0.041325 -0.331848 -0.051228  0.425265\n",
      "  -0.055962  0.002818  0.168613  0.065685 -0.363064 -0.177866  0.467434\n",
      "   0.231383  0.008063  0.022004 -0.071964 -0.487605  0.144436 -0.23375\n",
      "   0.118684 -0.023217  0.150676  0.277097 -0.317245 -0.003169 -0.371502\n",
      "  -0.46067   0.260743 -0.39796   0.451454  0.600397  0.264396  0.082088\n",
      "  -0.359662 -0.083587  0.118629 -0.191339 -0.426663  0.094797 -0.143404\n",
      "  -0.291897 -0.306675  0.065473  0.132556  0.182304 -0.011162 -0.13044\n",
      "  -0.679765 -0.185442 -0.344062 -0.024871 -0.482761  0.213686  0.092747\n",
      "  -0.215536  0.063152  0.160242  0.225541 -0.414093  0.21096  -0.04556\n",
      "  -0.036675  0.163579  0.15919   0.144838  0.152838  0.613607 -0.394284\n",
      "   0.104453 -0.396637 -0.186725 -0.416278  0.486712  0.198881 -0.028462\n",
      "  -0.426364 -0.48366   0.177266  0.077713 -0.401149  0.211246 -0.055804\n",
      "  -0.304603 -0.111279  0.367681 -0.308823 -0.202794 -0.093403  0.271166\n",
      "  -0.287128  0.348739  0.18378   0.060444  0.18594  -0.18708  -0.604159\n",
      "   0.078091 -0.008846  0.341394 -0.33319  -0.331632 -0.097969  0.025911\n",
      "   0.348432  0.12061   0.206594  0.16037   0.263815 -0.388845  0.286635\n",
      "   0.035278 -0.257006 -0.478897 -0.452332 -0.316331 -0.042176  0.20468\n",
      "   0.332977  0.168741 -0.497454  0.110033 -0.321234 -0.353042 -0.02474\n",
      "  -0.062366 -0.232836  0.03957   0.014688 -0.011113  0.537261]]\n"
     ]
    }
   ],
   "source": [
    "##查询词向量\n",
    "test_token_embedding = token_embedding.search(\"祖国\")\n",
    "print(test_token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b509d60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score1: 0.4686648\n",
      "score2: 0.597433\n"
     ]
    }
   ],
   "source": [
    "##计算两个词之间的相关性问题（余弦距离 欧式距离等）\n",
    "score1 = token_embedding.cosine_sim(\"祖国\",\"爱国\")\n",
    "score2 = token_embedding.cosine_sim(\"男孩\", \"女孩\")\n",
    "print('score1:', score1)\n",
    "print('score2:', score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "271bc12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##采用百度自家工具visual显示词向量分布,当然还可以采用sklean高维到低维显示 Sklearn中TSNE tensorboard也是可以可视化\n",
    "labels = token_embedding.vocab.to_tokens(list(range(0, 10000)))\n",
    "test_token_embedding = token_embedding.search(labels)\n",
    "from visualdl import LogWriter\n",
    "\n",
    "with LogWriter(logdir='./token_hidi') as writer:\n",
    "    writer.add_embeddings(tag='test', mat=[i for i in test_token_embedding], metadata=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c235ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "![test_visualdl](images/test_visualdl.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9b68e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddlenlp\n",
    "\n",
    "\n",
    "class BoWModel(nn.Layer):\n",
    "    def __init__(self, embedder):\n",
    "        super().__init__()\n",
    "        self.embedder = embedder\n",
    "        emb_dim = self.embedder.embedding_dim\n",
    "        self.encoder = paddlenlp.seq2vec.BoWEncoder(emb_dim)\n",
    "        self.cos_sim_func = nn.CosineSimilarity(axis=-1)\n",
    "\n",
    "    def get_cos_sim(self, text_a, text_b):\n",
    "        text_a_embedding = self.forward(text_a)\n",
    "        text_b_embedding = self.forward(text_b)\n",
    "        cos_sim = self.cos_sim_func(text_a_embedding, text_b_embedding)\n",
    "        return cos_sim\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "\n",
    "        # Shape: (batch_size, embedding_dim)\n",
    "        summed = self.encoder(embedded_text)\n",
    "\n",
    "        return summed\n",
    "\n",
    "model = BoWModel(embedder=token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44760c22",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-db2ef01c4e31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tokenizer'"
     ]
    }
   ],
   "source": [
    "from data import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.set_vocab(vocab=token_embedding.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8260139f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "无",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
