{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8541ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-24 21:13:03--  https://dataset-bj.cdn.bcebos.com/qianyan/COTE-BD.zip\n",
      "正在解析主机 dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)... 114.232.92.35, 114.80.30.35\n",
      "正在连接 dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)|114.232.92.35|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度： 1182741 (1.1M) [application/zip]\n",
      "正在保存至: “COTE-BD.zip”\n",
      "\n",
      "COTE-BD.zip         100%[===================>]   1.13M  6.01MB/s    用时 0.2s    \n",
      "\n",
      "2021-06-24 21:13:03 (6.01 MB/s) - 已保存 “COTE-BD.zip” [1182741/1182741])\n",
      "\n",
      "--2021-06-24 21:13:03--  https://dataset-bj.cdn.bcebos.com/qianyan/COTE-MFW.zip\n",
      "正在解析主机 dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)... 114.232.92.35, 114.80.30.35\n",
      "正在连接 dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)|114.232.92.35|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度： 4872264 (4.6M) [application/zip]\n",
      "正在保存至: “COTE-MFW.zip”\n",
      "\n",
      "COTE-MFW.zip        100%[===================>]   4.65M  9.41MB/s    用时 0.5s    \n",
      "\n",
      "2021-06-24 21:13:04 (9.41 MB/s) - 已保存 “COTE-MFW.zip” [4872264/4872264])\n",
      "\n",
      "--2021-06-24 21:13:04--  https://dataset-bj.cdn.bcebos.com/qianyan/COTE-DP.zip\n",
      "正在解析主机 dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)... 114.232.92.35, 114.80.30.35\n",
      "正在连接 dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)|114.232.92.35|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度： 4220083 (4.0M) [application/zip]\n",
      "正在保存至: “COTE-DP.zip”\n",
      "\n",
      "COTE-DP.zip         100%[===================>]   4.02M  9.17MB/s    用时 0.4s    \n",
      "\n",
      "2021-06-24 21:13:04 (9.17 MB/s) - 已保存 “COTE-DP.zip” [4220083/4220083])\n",
      "\n",
      "Archive:  ./COTE-BD.zip\n",
      "   creating: ./data/COTE-BD/\n",
      "  inflating: ./data/COTE-BD/train.tsv  \n",
      "   creating: ./data/__MACOSX/\n",
      "   creating: ./data/__MACOSX/COTE-BD/\n",
      "  inflating: ./data/__MACOSX/COTE-BD/._train.tsv  \n",
      "  inflating: ./data/COTE-BD/License.pdf  \n",
      "  inflating: ./data/__MACOSX/COTE-BD/._License.pdf  \n",
      "  inflating: ./data/COTE-BD/test.tsv  \n",
      "  inflating: ./data/__MACOSX/COTE-BD/._test.tsv  \n",
      "  inflating: ./data/__MACOSX/._COTE-BD  \n",
      "Archive:  ./COTE-MFW.zip\n",
      "   creating: ./data/COTE-MFW/\n",
      "  inflating: ./data/COTE-MFW/train.tsv  \n",
      "   creating: ./data/__MACOSX/COTE-MFW/\n",
      "  inflating: ./data/__MACOSX/COTE-MFW/._train.tsv  \n",
      "  inflating: ./data/COTE-MFW/License.pdf  \n",
      "  inflating: ./data/__MACOSX/COTE-MFW/._License.pdf  \n",
      "  inflating: ./data/COTE-MFW/test.tsv  \n",
      "  inflating: ./data/__MACOSX/COTE-MFW/._test.tsv  \n",
      "  inflating: ./data/__MACOSX/._COTE-MFW  \n",
      "Archive:  ./COTE-DP.zip\n",
      "   creating: ./data/COTE-DP/\n",
      "  inflating: ./data/COTE-DP/train.tsv  \n",
      "   creating: ./data/__MACOSX/COTE-DP/\n",
      "  inflating: ./data/__MACOSX/COTE-DP/._train.tsv  \n",
      "  inflating: ./data/COTE-DP/License.pdf  \n",
      "  inflating: ./data/__MACOSX/COTE-DP/._License.pdf  \n",
      "  inflating: ./data/COTE-DP/test.tsv  \n",
      "  inflating: ./data/__MACOSX/COTE-DP/._test.tsv  \n",
      "  inflating: ./data/__MACOSX/._COTE-DP  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#下载数据集\n",
    "!wget https://dataset-bj.cdn.bcebos.com/qianyan/COTE-BD.zip\n",
    "!wget https://dataset-bj.cdn.bcebos.com/qianyan/COTE-MFW.zip\n",
    "!wget https://dataset-bj.cdn.bcebos.com/qianyan/COTE-DP.zip\n",
    "\n",
    "# 解压数据集到 ./data 目录\n",
    "!unzip ./COTE-BD.zip -d ./data/\n",
    "!unzip ./COTE-MFW.zip -d ./data/\n",
    "!unzip ./COTE-DP.zip -d ./data/\n",
    "\n",
    "# 删除压缩包\n",
    "!rm COTE-BD.zip\n",
    "!rm COTE-MFW.zip\n",
    "!rm COTE-DP.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "989e87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到数据集字典\n",
    "def open_func(file_path):\n",
    "    return [line.strip() for line in open(file_path, 'r', encoding='utf8').readlines()[1:] if len(line.strip().split('\\t')) >= 2]\n",
    "\n",
    "data_dict = {'cotebd': {'test': open_func('data/COTE-BD/test.tsv'),\n",
    "                        'train': open_func('data/COTE-BD/train.tsv')},\n",
    "             'cotedp': {'test': open_func('data/COTE-DP/test.tsv'),\n",
    "                        'train': open_func('data/COTE-DP/train.tsv')},\n",
    "             'cotemfw': {'test': open_func('data/COTE-MFW/test.tsv'),\n",
    "                        'train': open_func('data/COTE-MFW/train.tsv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e0b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义数据集\n",
    "from paddle.io import Dataset, DataLoader\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "import numpy as np\n",
    "label_list = {'B': 0, 'I': 1, 'O': 2}\n",
    "index2label = {0: 'B', 1: 'I', 2: 'O'}\n",
    "\n",
    "# 考虑token_type_id\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512, for_test=False):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "        self._for_test = for_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        samples = self._data[idx].split('\\t')\n",
    "        label = samples[-2]\n",
    "        text = samples[-1]\n",
    "        if self._for_test:\n",
    "            origin_enc = self._tokenizer.encode(text, max_seq_len=self._max_len)['input_ids']\n",
    "            return np.array(origin_enc, dtype='int64')\n",
    "        else:\n",
    "            \n",
    "            # 由于并不是每个字都是一个token，这里采用一种简单的处理方法，先编码label，再编码text中除了label以外的词，最后合到一起\n",
    "            texts = text.split(label)\n",
    "            label_enc = self._tokenizer.encode(label)['input_ids']\n",
    "            cls_enc = label_enc[0]\n",
    "            sep_enc = label_enc[-1]\n",
    "            label_enc = label_enc[1:-1]\n",
    "            \n",
    "            # 合并\n",
    "            origin_enc = []\n",
    "            label_ids = []\n",
    "            for index, text in enumerate(texts):\n",
    "                text_enc = self._tokenizer.encode(text)['input_ids']\n",
    "                text_enc = text_enc[1:-1]\n",
    "                origin_enc += text_enc\n",
    "                label_ids += [label_list['O']] * len(text_enc)\n",
    "                if index != len(texts) - 1:\n",
    "                    origin_enc += label_enc\n",
    "                    label_ids += [label_list['B']] + [label_list['I']] * (len(label_enc) - 1)\n",
    "\n",
    "            origin_enc = [cls_enc] + origin_enc + [sep_enc]\n",
    "            label_ids = [label_list['O']] + label_ids + [label_list['O']]\n",
    "            \n",
    "            # 截断\n",
    "            if len(origin_enc) > self._max_len:\n",
    "                origin_enc = origin_enc[:self._max_len-1] + origin_enc[-1:]\n",
    "                label_ids = label_ids[:self._max_len-1] + label_ids[-1:]\n",
    "            return np.array(origin_enc, dtype='int64'), np.array(label_ids, dtype='int64')\n",
    "\n",
    "\n",
    "def batchify_fn(for_test=False):\n",
    "    if for_test:\n",
    "        return lambda samples, fn=Pad(axis=0, pad_val=tokenizer.pad_token_id): np.row_stack([data for data in fn(samples)])\n",
    "    else:\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\n",
    "                                        Pad(axis=0, pad_val=label_list['O'])): [data for data in fn(samples)]\n",
    "\n",
    "\n",
    "def get_data_loader(data, tokenizer, batch_size=32, max_len=512, for_test=False):\n",
    "    dataset = MyDataset(data, tokenizer, max_len, for_test)\n",
    "    shuffle = True if not for_test else False\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3914d214",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SkepForTokenClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-370d713dace2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 模型和分词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSkepForTokenClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skep_ernie_1.0_large_ch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSkepTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skep_ernie_1.0_large_ch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SkepForTokenClassification' is not defined"
     ]
    }
   ],
   "source": [
    "# 模型搭建\n",
    "\n",
    "# 载入模型和Tokenizer\n",
    "import paddlenlp\n",
    "from paddlenlp.transformers import SkepForTokenClassification, SkepTokenizer\n",
    "import paddle\n",
    "from paddle.static import InputSpec\n",
    "from paddlenlp.metrics import Perplexity\n",
    "\n",
    "# 模型和分词\n",
    "model = SkepForTokenClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=3)\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')\n",
    "\n",
    "# 参数设置\n",
    "data_name = 'cotemfw'  # 更改此选项改变数据集\n",
    "\n",
    "## 训练相关\n",
    "epochs = 1\n",
    "learning_rate = 2e-5\n",
    "batch_size = 8\n",
    "max_len = 512\n",
    "\n",
    "## 数据相关\n",
    "train_dataloader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_len, for_test=False)\n",
    "\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\n",
    "label = InputSpec((-1, -1, 3), dtype='int64', name='label')\n",
    "model = paddle.Model(model, [input], [label])\n",
    "\n",
    "# 模型准备\n",
    "\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\n",
    "model.prepare(optimizer, loss=paddle.nn.CrossEntropyLoss(), metrics=[Perplexity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68e569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
